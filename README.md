# multimodal-vision-language-model
ViT-based Visionâ€“Language Model for semantic image search using contrastive learning
# Multimodal Visionâ€“Language Model (VLM)

This project implements a ViT-based Visionâ€“Language Model trained using contrastive learning on the Flickr8k dataset.

## ğŸš€ Features
- Vision Transformer (ViT) image encoder
- Transformer-based text encoder (DistilBERT)
- Contrastive learning for imageâ€“text alignment
- Semantic image search
- Imageâ€“text matching

## ğŸ§  Architecture
- Image Encoder: ViT-Base (google/vit-base-patch16-224)
- Text Encoder: DistilBERT
- Shared embedding space using projection heads
- CLIP-style contrastive loss

## ğŸ“Š Dataset
- Flickr8k
- 8,092 images
- ~40,460 imageâ€“caption pairs

## ğŸ” Applications
- Semantic image search using natural language queries
- Imageâ€“text similarity scoring

## ğŸ›  Tech Stack
- PyTorch
- HuggingFace Transformers
- Kaggle GPU

## ğŸ“Œ Example Query
